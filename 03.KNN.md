# KNN (K-Nearest Neighbors)

## O que é KNN?

KNN, ou K-Nearest Neighbors (K-Vizinhos Mais Próximos), é um algoritmo de aprendizado de máquina simples e versátil, usado para classificação e regressão. Ele é considerado um "lazy learner" (aprendiz preguiçoso) porque não constrói um modelo durante a fase de treinamento. Em vez disso, ele armazena todo o conjunto de dados de treinamento e a etapa de "aprendizagem" só ocorre quando uma nova previsão é solicitada. A ideia principal é que um ponto de dados é classificado pela maioria de seus vizinhos mais próximos.

## Qual é o Grande Objetivo?

O objetivo do KNN é encontrar um grupo de k vizinhos (pontos de dados) no conjunto de treinamento que estejam mais próximos de um novo ponto de dados, ainda não classificado. Uma vez que esses vizinhos são identificados, a classificação do novo ponto é determinada pelo voto da maioria das classes entre esses k vizinhos.

## Como o Modelo Encontra a Solução?

A "solução" do KNN é a classificação do novo ponto de dados. Para encontrar essa solução, o modelo segue estes passos:

1. Escolha de k: Primeiro, é preciso escolher um valor para k, que é o número de vizinhos a serem considerados.
2. Cálculo da Distância: O modelo calcula a distância do novo ponto de dados para todos os pontos de dados no conjunto de treinamento. As distâncias mais comuns são a Euclidiana (a distância em linha reta) e a de Manhattan.
3. Identificação dos Vizinhos: Os pontos de dados do conjunto de treinamento são classificados com base nessas distâncias em ordem crescente.
4. Voto da Maioria: O modelo seleciona os k vizinhos que estão mais próximos do novo ponto. A classe do novo ponto é então determinada pela classe mais frequente entre esses k vizinhos.

## Como o Modelo Avalia seu Desempenho?

O KNN não tem uma fase de treinamento tradicional para ajustar parâmetros. Em vez disso, seu desempenho é avaliado pela sua capacidade de classificar corretamente os pontos de dados de teste. A avaliação depende principalmente de duas coisas:

- A Escolha de k: Um k muito pequeno pode tornar o modelo sensível a ruídos e outliers, levando a classificações incorretas. Um k muito grande pode suavizar a fronteira de decisão, misturando classes e diminuindo a precisão, especialmente em classes com poucas amostras.
- A Métrica de Distância: A escolha da métrica de distância (por exemplo, Euclidiana, Manhattan) é crucial e pode impactar o desempenho do modelo, pois determina o que significa "próximo".

## Limitações do KNN
A principal fraqueza do KNN é sua ineficiência computacional e de memória, especialmente com grandes conjuntos de dados. Como o modelo precisa calcular a distância de um novo ponto para todos os pontos do conjunto de treinamento, ele se torna muito lento à medida que o tamanho dos dados aumenta. Além disso, o KNN é sensível à escala dos dados e a features (atributos) irrelevantes. Se as features tiverem escalas muito diferentes, aquelas com escalas maiores podem dominar a distância, mesmo que sejam menos importantes. Por isso, a normalização dos dados é uma etapa fundamental antes de usar o KNN.

## Avaliando o Desempenho do Modelo
Para avaliar o desempenho de um modelo KNN, usamos métricas comuns de classificação. O desempenho geral do modelo é geralmente medido por:

- Acurácia: A porcentagem de classificações corretas.
- Matriz de Confusão: Uma tabela que mostra o número de verdadeiros positivos, verdadeiros negativos, falsos positivos e falsos negativos.
- Precisão, Recall e F1-Score: Métricas que fornecem um panorama mais detalhado do desempenho, especialmente em casos de desbalanceamento de classes.
- Validação Cruzada (Cross-Validation): Uma técnica para testar a performance do modelo em diferentes subconjuntos de dados, garantindo que o desempenho não seja apenas resultado de uma divisão de dados específica.
