# Gradiente Descendente

O Gradiente Descendente é um algoritmo de otimização poderoso, usado para encontrar a "melhor solução" em problemas onde não há uma fórmula direta para isso. Ele é especialmente útil em modelos de Machine Learning como a Regressão Logística e Redes Neurais.

## A Ideia Principal
Imagine que você está no topo de uma montanha em um nevoeiro denso e seu objetivo é chegar ao ponto mais baixo. Você não consegue ver o caminho todo, então a única estratégia é dar um passo em direção à descida mais íngreme. Você repete esse processo, ajustando sua direção a cada passo, até que não consiga mais descer.

É exatamente assim que o Gradiente Descendente funciona.

## O "Ponto Inicial" do Modelo

O algoritmo começa com uma suposição inicial para os parâmetros do seu modelo, como a inclinação e o intercepto de uma linha. Isso é como escolher um ponto aleatório para começar sua caminhada na montanha.

## A "Função de Custo" (Sua Montanha)
Para saber se o modelo está "bom" ou "ruim" em sua suposição inicial, ele usa uma Função de Custo. No exemplo da Regressão Linear, essa função é a Soma dos Resíduos Quadrados (SSR).

- Resíduos: A diferença entre o valor que o modelo previu e o valor real do dado.
- SSR: A soma de todos esses resíduos elevados ao quadrado.

A SSR mede a distância total entre a linha do modelo e os seus dados. Um SSR alto significa que a linha está "longe" dos pontos, ou seja, o modelo está "ruim". O objetivo é minimizar a SSR, ou seja, chegar ao ponto mais baixo da sua "montanha".

## O "Gradiente" (A Direção da Descida)

O Gradiente é a derivada da Função de Custo em relação a cada um dos parâmetros do modelo. O que isso significa em termos simples?

- O gradiente nos diz a direção do "pulo" que o modelo precisa dar para encontrar a inclinação mais íngreme da montanha.
- A magnitude do gradiente nos diz o quão íngreme é essa inclinação. Um gradiente grande significa que o modelo está muito longe da solução ideal, então ele pode dar um passo maior. Um gradiente pequeno significa que o modelo está perto do ponto mais baixo, então ele precisa dar um passo menor e mais cuidadoso.

## O Processo Iterativo

1. O modelo faz uma suposição inicial para os parâmetros (ex: intercepto = 0).
2. Ele calcula o SSR para essa suposição e depois encontra o gradiente (a inclinação da "montanha" nesse ponto).
3. Ele dá um pequeno passo na direção oposta ao gradiente (ladeira abaixo), ajustando os parâmetros.
4. Ele repete o processo, recalculando o SSR e o gradiente, e dando outro passo.

Esse ciclo se repete até que o modelo atinja um ponto em que o gradiente seja quase zero (o ponto mais baixo da montanha) ou atinja um número máximo de passos. O resultado final é o conjunto de parâmetros que minimiza a Função de Custo e oferece a melhor solução para o problema.

## Gradiente Descendente para Regressão Logística

Embora o exemplo acima use a Regressão Linear, a ideia é a mesma para a Regressão Logística. Em vez de minimizar a SSR, ele maximiza a Verossimilhança (ou minimiza o logaritmo negativo da verossimilhança), seguindo a mesma lógica de "descer a montanha" para encontrar a solução ideal.

## O Processo do Algoritmo:
1. Ponto de Partida: O algoritmo começa com valores aleatórios para os parâmetros do modelo.
2. Cálculo do Gradiente: Ele calcula o gradiente da Função de Custo nesse ponto para saber qual a direção de "descida".
3. Ajuste de Parâmetros: Os parâmetros do modelo são atualizados com base no gradiente, movendo-se na direção oposta a ele. Isso leva a um passo em direção ao ponto de menor erro.
4. Repetição: Os passos 2 e 3 são repetidos em um processo iterativo até que o modelo chegue a um ponto onde o erro seja mínimo, o que é o ponto mais baixo da "montanha".
